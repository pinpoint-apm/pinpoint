#
# Pinpoint agent configuration
#
# Profiles
#  - -Dpinpoint.profiler.profiles.active=release or local
#  - Modify `pinpoint.profiler.profiles.active=release` in $PINPOINT_AGENT_DIR/pinpoint-root.config
# Custom Profile
#  - 1. Create a custom profile in $PINPOINT_HOME/profiles/MyProfile
#    - Add pinpoint.config & log4j2.xml
#  - 2. -Dpinpoint.profiler.profiles.active=MyProfile
# Support external property
#  - -Dpinpoint.config=$MY_CONFIG_PATH/pinpoint.config
pinpoint.profiler.profiles.active=release

# GRPC or THRIFT
profiler.transport.module=GRPC
###########################################################
# gRPC Configuration                                      #
###########################################################
profiler.transport.grpc.collector.ip=127.0.0.1

# subconnection expiring loadbalancer
profiler.transport.grpc.loadbalancer.renew.period.millis=3153600000000

# placeHolder support "${key}"
# Agent
profiler.transport.grpc.agent.collector.ip=${profiler.transport.grpc.collector.ip}
profiler.transport.grpc.agent.collector.port=9991
profiler.transport.grpc.agent.ssl.enable=false
profiler.transport.grpc.agent.sender.channel.executor.queue.size=1000
profiler.transport.grpc.agent.sender.request.timeout.millis=6000
profiler.transport.grpc.agent.sender.keepalive.time.millis=30000
profiler.transport.grpc.agent.sender.keepalive.timeout.millis=60000
profiler.transport.grpc.agent.sender.loadbalancer=pick_first
profiler.transport.grpc.agent.sender.connect.timeout.millis=3000
profiler.transport.grpc.agent.sender.headers.size.max=8K
profiler.transport.grpc.agent.sender.message.inbound.size.max=4M
profiler.transport.grpc.agent.sender.flow-control.window.size=1M
profiler.transport.grpc.agent.sender.write.buffer.highwatermark=32M
profiler.transport.grpc.agent.sender.write.buffer.lowwatermark=16M
## AUTO, NIO, EPOLL
profiler.transport.grpc.agent.sender.channel-type=AUTO
profiler.transport.grpc.agent.sender.maxtraceevent=0

# Metadata
profiler.transport.grpc.metadata.collector.ip=${profiler.transport.grpc.collector.ip}
profiler.transport.grpc.metadata.collector.port=9991
profiler.transport.grpc.metadata.ssl.enable=false
profiler.transport.grpc.metadata.sender.executor.queue.size=1000
profiler.transport.grpc.metadata.sender.channel.executor.queue.size=1000
profiler.transport.grpc.metadata.sender.request.timeout.millis=6000
profiler.transport.grpc.metadata.sender.retry.max.count=3
profiler.transport.grpc.metadata.sender.retry.delay.millis=1000
profiler.transport.grpc.metadata.sender.keepalive.time.millis=30000
profiler.transport.grpc.metadata.sender.keepalive.timeout.millis=60000
profiler.transport.grpc.metadata.sender.loadbalancer=pick_first
profiler.transport.grpc.metadata.sender.connect.timeout.millis=3000
profiler.transport.grpc.metadata.sender.headers.size.max=8K
profiler.transport.grpc.metadata.sender.message.inbound.size.max=4M
profiler.transport.grpc.metadata.sender.flow-control.window.size=1M
profiler.transport.grpc.metadata.sender.write.buffer.highwatermark=32M
profiler.transport.grpc.metadata.sender.write.buffer.lowwatermark=16M
##  AUTO, NIO, EPOLL
profiler.transport.grpc.metadata.sender.channel-type=AUTO
profiler.transport.grpc.metadata.sender.maxtraceevent=0

# Stat
profiler.transport.grpc.stat.collector.ip=${profiler.transport.grpc.collector.ip}
profiler.transport.grpc.stat.collector.port=9992
profiler.transport.grpc.stat.ssl.enable=false
profiler.transport.grpc.stat.sender.executor.queue.size=1000
profiler.transport.grpc.stat.sender.channel.executor.queue.size=1000
profiler.transport.grpc.stat.sender.request.timeout.millis=6000
profiler.transport.grpc.stat.sender.keepalive.time.millis=30000
profiler.transport.grpc.stat.sender.keepalive.timeout.millis=60000
profiler.transport.grpc.stat.sender.loadbalancer=pick_first
profiler.transport.grpc.stat.sender.connect.timeout.millis=3000
profiler.transport.grpc.stat.sender.headers.size.max=8K
profiler.transport.grpc.stat.sender.message.inbound.size.max=4M
profiler.transport.grpc.stat.sender.flow-control.window.size=1M
profiler.transport.grpc.stat.sender.write.buffer.highwatermark=32M
profiler.transport.grpc.stat.sender.write.buffer.lowwatermark=16M
##  AUTO, NIO, EPOLL
profiler.transport.grpc.stat.sender.channel-type=AUTO
profiler.transport.grpc.stat.sender.maxtraceevent=0

# Span
profiler.transport.grpc.span.collector.ip=${profiler.transport.grpc.collector.ip}
profiler.transport.grpc.span.collector.port=9993
profiler.transport.grpc.span.ssl.enable=false
profiler.transport.grpc.span.sender.executor.queue.size=1000
profiler.transport.grpc.span.sender.channel.executor.queue.size=1000
profiler.transport.grpc.span.sender.request.timeout.millis=6000
profiler.transport.grpc.span.sender.keepalive.time.millis=30000
profiler.transport.grpc.span.sender.keepalive.timeout.millis=60000
profiler.transport.grpc.span.sender.loadbalancer=pick_first
profiler.transport.grpc.span.sender.connect.timeout.millis=3000
profiler.transport.grpc.span.sender.headers.size.max=8K
profiler.transport.grpc.span.sender.message.inbound.size.max=4M
profiler.transport.grpc.span.sender.flow-control.window.size=1M
profiler.transport.grpc.span.sender.write.buffer.highwatermark=32M
profiler.transport.grpc.span.sender.write.buffer.lowwatermark=16M
profiler.transport.grpc.span.sender.discardpolicy.logger.discard.ratelimit=1
profiler.transport.grpc.span.sender.discardpolicy.maxpendingthreshold=1024
profiler.transport.grpc.span.sender.discardpolicy.discard-count-for-reconnect=1000
profiler.transport.grpc.span.sender.discardpolicy.not-ready-timeout-millis=300000
profiler.transport.grpc.span.sender.rpc.age.max.millis=3153600000000
##  AUTO, NIO, EPOLL
profiler.transport.grpc.span.sender.channel-type=AUTO
profiler.transport.grpc.span.sender.maxtraceevent=8
profiler.transport.grpc.span.sender.limitcount=100
profiler.transport.grpc.span.sender.limittime=60000

# Grpc Tls
profiler.transport.grpc.ssl.enable=false
# openssl or jdk (jdk requires 1.9 or above)
profiler.transport.grpc.ssl.provider.type=jdk
# please insert .crt file path (If empty, default Root CA certificates will be used.(generally, from JAVA_HOME/lib/cacerts))
# (prefix for claspath = claspath:, prefix for absoulute path = file:)
profiler.transport.grpc.ssl.trust.cert.file.path=

# This configuration enable some function of netty
# Functions are available without the this configuration when using jdk8 and below,
profiler.system.property.io.netty.tryReflectionSetAccessible=true
# disable netty directbuffer
profiler.system.property.io.netty.noPreferDirect=false

###########################################################
# Thrift Configuration                                    #
###########################################################
profiler.collector.ip=127.0.0.1

# placeHolder support "${key}"
profiler.collector.span.ip=${profiler.collector.ip}
profiler.collector.span.port=9996

# placeHolder support "${key}"
profiler.collector.stat.ip=${profiler.collector.ip}
profiler.collector.stat.port=9995

# placeHolder support "${key}"
profiler.collector.tcp.ip=${profiler.collector.ip}
profiler.collector.tcp.port=9994

###########################################################
# Profiler Global Configuration                           #
###########################################################
profiler.enable=true

# Application namespace
# Differentiate from external pinpoint agents. (e.g., com.pinpoint)
profiler.application.namespace=

profiler.interceptorregistry.size=8192

# Manually override jvm vendor name (Oracle, IBM, OpenJDK, etc)
# You probably won't ever need to set this value.
profiler.jvm.vendor.name=

# Manually override agent's OS name (MAC, Window, Linus, AIX, HP_UX, BSD)
# You probably won't ever need to set this value.(to collect file descriptor)
profiler.os.name=

# Interval (in milliseconds) at which agent stat data is collected. (default : 5000, min : 1000, max : 5000)
profiler.jvm.stat.collect.interval=5000
# Number of agent stat data sent to the collector in a single batch. (default : 6)
profiler.jvm.stat.batch.send.count=6

# Allow to add detailed collector's metrics
profiler.jvm.stat.collect.detailed.metrics=true

# Allow sampling.
profiler.sampling.enable=true

# support 2 types, COUNTING(default) and PERCENT.
# If this value set to COUNTING(default), sampling rate is 1/n.
# If this value set to PERCENT, sampling rate is n%.
profiler.sampling.type=COUNTING

# depend on profiler.samplging.rate.type,

# if it's COUNTING(the default), then 1 out of n transactions will be sampled where n is the rate.
# eg. 1: 100%     20: 5%     50: 2%      100: 1%
profiler.sampling.counting.sampling-rate=1
# @Deprecate : Alias for profiler.sampling.counting.sampling-rate
#profiler.sampling.rate=1

# if it's PERCENT, then first x transactions out of y transactions will be sampled.
# Support from 100% to 0.01%
# eg. 100: 100%    50: 50%   5: 5%  0.01: 0.01%
profiler.sampling.percent.sampling-rate=100

# Permits per second, if throughput is 0, it is unlimited.
# "New" is a transaction that is newly traced.
profiler.sampling.new.throughput=0
# "Continue" is a transaction that is already being tracked.
profiler.sampling.continue.throughput=0

# Allow buffering when flushing span to IO.
profiler.io.buffering.enable=true

# How many spans to store if buffering enabled.
profiler.io.buffering.buffersize=20

# Maximum number of log directories
# - $PINPOINT_AGENT_DIR/logs/${AGENT_ID}
profiler.logdir.maxbackupsize=5

###########################################################
# Base Tcp Sender                                        #
###########################################################
# Allow TCP data command.
profiler.tcpdatasender.command.accept.enable=true
profiler.tcpdatasender.command.activethread.enable=true
profiler.tcpdatasender.command.activethread.count.enable=true
profiler.tcpdatasender.command.activethread.threaddump.enable=true
profiler.tcpdatasender.command.activethread.threadlightdump.enable=true

profiler.tcpdatasender.client.write.timeout=3000
profiler.tcpdatasender.client.request.timeout=3000
profiler.tcpdatasender.client.reconnect.interval=3000
profiler.tcpdatasender.client.ping.interval=300000
profiler.tcpdatasender.client.handshake.interval=60000

profiler.tcpdatasender.client.write.buffer.highwatermark=32m
profiler.tcpdatasender.client.write.buffer.lowwatermark=16m

# Interval to retry sending agent info. Unit is milliseconds.
profiler.agentInfo.send.retry.interval=300000


###########################################################
# Stat Data Sender                                        #
###########################################################
# Should keep in mind
# 1. Loadbancing : TCP transport load balancing is per connection.(UDP transport loadbalancing is per packet)
# 2. In unexpected situations, UDP has its own protection feature (like packet loss etc.), but tcp does not have such a feature. (We will add protection later)
profiler.statdatasender.transport.type=UDP

# These settings are active only when using UDP.
# Capacity of the StatDataSender write queue.
profiler.statdatasender.write.queue.size=5120
#profiler.statdatasender.socket.sendbuffersize=1048576
#profiler.statdatasender.socket.timeout=3000
profiler.statdatasender.chunk.size=16384
profiler.statdatasender.socket.type=OIO

# These settings are active only when using TCP.
profiler.statdatasender.write.buffer.highwatermark=16m
profiler.statdatasender.write.buffer.lowwatermark=8m


###########################################################
# Span Data Sender                                        #
###########################################################
# Should keep in mind
# 1. Loadbancing : TCP transport load balancing is per connection.(UDP transport loadbalancing is per packet)
# 2. In unexpected situations, UDP has its own protection feature (like packet loss etc.), but tcp does not have such a feature. (We will add protection later)
profiler.spandatasender.transport.type=UDP

# These settings are active only when using UDP.
# Capacity of the SpanDataSender write queue.
profiler.spandatasender.write.queue.size=5120
#profiler.spandatasender.socket.sendbuffersize=1048576
#profiler.spandatasender.socket.timeout=3000
profiler.spandatasender.chunk.size=16384
profiler.spandatasender.socket.type=OIO

# These settings are active only when using TCP.
profiler.spandatasender.write.buffer.highwatermark=16m
profiler.spandatasender.write.buffer.lowwatermark=8m

#profiler.pinpoint.base-package=com/navercorp/pinpoint/
#profiler.pinpoint.exclude-package=web/,sdk/

# Trace Agent active thread info.
profiler.pinpoint.activethread=true

# Trace DataSource
profiler.pinpoint.datasource=true

# Deadlock Monitor
profiler.monitor.deadlock.enable=true
profiler.monitor.deadlock.interval=60000

## Call Stack
# Set max depth, if -1 is unlimited and min is 2.
profiler.callstack.max.depth=64

# Set max sequence, if -1 is unlimited and min is 4.
profiler.callstack.max.sequence=5000

# weather or not to propagate exceptions occurred at interceptor
profiler.interceptor.exception.propagate=false

# Allow bytecode framework (ASM only)
profiler.instrument.engine=ASM

profiler.instrument.jdk.allow.classnames=java.util.concurrent.CompletableFuture,java.lang.ProcessBuilder,java.util.function.Supplier

# bytecode dump option
# java bytecode debug option
bytecode.dump.enable=false
#bytecode.dump.classlist=com.pinpoint.user.UserService,com.pinpoint.debug.TestClass
bytecode.dump.classlist=
bytecode.dump.bytecode=false
bytecode.dump.verify=false
bytecode.dump.asm=false

# Matcher
profiler.instrument.matcher.enable=true
# Matcher cache. max size is 64.
profiler.instrument.matcher.interface.cache.size=4
profiler.instrument.matcher.interface.cache.entry.size=16
profiler.instrument.matcher.annotation.cache.size=4
profiler.instrument.matcher.annotation.cache.entry.size=4
profiler.instrument.matcher.super.cache.size=4
profiler.instrument.matcher.super.cache.entry.size=4

# Lambda expressions.
profiler.lambda.expressions.support=true

# Proxy HTTP headers.
# Please see (https://github.com/naver/pinpoint/blob/master/doc/proxy-http-header.md) for more information.
profiler.proxy.http.header.enable=true

# HTTP status code with request failure.
# 1xx, 2xx, 3xx, 4xx, 5xx, 100, 101, 200, 201, ... 501, 502, 503, 504, 505
# e.g. profiler.http.status.code.errors=5xx, 401, 403
profiler.http.status.code.errors=5xx

# record HTTP request headers case-sensitive
# set to HEADERS-ALL to record all of the request headers including cookie.
# e.g. profiler.http.record.request.headers=X-AccessKey,X-Device-UUID
#profiler.http.record.request.headers=

# record HTTP request cookies(case-sensitive) in Cookie header
# e.g. profiler.http.record.request.cookies=userid,device-id,uuid
#profiler.http.record.request.cookies=

# record HTTP response headers case-sensitive
# set to HEADERS-ALL to record all of the headers.
# e.g. profiler.http.record.response.headers=Set-Cookie
#profiler.http.record.response.headers=

###########################################################
# Application Type                                        #
###########################################################
#profiler.applicationservertype=TOMCAT
#profiler.applicationservertype=BLOC

# Needs to be a comma separated list of plugin's groupId:artifactId
# Ex: com.navercorp.pinpoint:pinpoint-tomcat-plugin, com.navercorp.pinpoint:pinpoint-jboss-plugin
profiler.plugin.load.order=
profiler.plugin.disable=

###########################################################
# SERVER                                                  #
###########################################################
# Server integration settings.
# Spring WebFlux(TOMCAT, JETTY, UNDERTOW, REACTOR-NETTY), JBOSS, RESIN, WEBLOGIC, VERT.X, WEBSPHERE, AKKA
# Individual settings take precedence.
# For example, setting "profiler.tomcat.excludeurl=A" takes precedence over setting "profiler.server.excludeurl=B".

# Hide pinpoint headers.
profiler.server.hidepinpointheader=true
# URLs to exclude from tracing.
# Support ant style pattern. e.g. /aa/*.html, /??/exclude.html
profiler.server.excludeurl=
# HTTP Request methods to exclude from tracing.
# e.g. POST, PUT
profiler.server.excludemethod=
# Trace request param.
profiler.server.tracerequestparam=true
# Original IP address header. e.g. X-Forwarded-For or X-Real-IP
# https://en.wikipedia.org/wiki/X-Forwarded-For
profiler.server.realipheader=
# optional parameter, If the header value is ${profiler.server.realipemptyvalue}, Ignore header value.
profiler.server.realipemptyvalue=

###########################################################
# BANNER                                                  #
###########################################################
# Pinpoint Banner Settings
# Pinpoint banner mode : OFF, CONSOLE, LOG
pinpoint.banner.mode=console
pinpoint.banner.configs=pinpoint.profiler.profiles.active,\
                        pinpoint.applicationName,\
                        pinpoint.agentId,\
                        pinpoint.agentName,\
                        profiler.transport.module,\
                        profiler.transport.grpc.collector.ip,\
                        profiler.transport.grpc.agent.collector.port,\
                        profiler.transport.grpc.metadata.collector.port,\
                        profiler.transport.grpc.stat.collector.port,\
                        profiler.transport.grpc.span.collector.port,\
                        profiler.collector.ip,\
                        profiler.collector.span.port,\
                        profiler.collector.stat.port,\
                        profiler.collector.tcp.port,\
                        profiler.spandatasender.transport.type,\
                        profiler.statdatasender.transport.type,\
                        profiler.sampling.enable,\
                        profiler.sampling.type,\
                        profiler.sampling.counting.sampling-rate,\
                        profiler.sampling.percent.sampling-rate