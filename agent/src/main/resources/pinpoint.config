#
# Pinpoint agent configuration
#
# Profiles
#  - -Dpinpoint.profiler.profiles.active=release or local
#  - Modify `pinpoint.profiler.profiles.active=release` in $PINPOINT_AGENT_DIR/pinpoint.config
# Custom Profile
#  - 1. Create a custom profile in $PINPOINT_HOME/profiles/MyProfile
#    - Add pinpoint-env.config & log4j.xml
#  - 2. -Dpinpoint.profiler.profiles.active=MyProfile
# Support external property
#  - -Dpinpoint.config=$MY_CONFIG_PATH/pinpoint.config
pinpoint.profiler.profiles.active=release

# GRPC or THRIFT
profiler.transport.module=GRPC
###########################################################
# gRPC Configuration                                      #
###########################################################
profiler.transport.grpc.collector.ip=127.0.0.1

# placeHolder support "${key}"
# Agent
profiler.transport.grpc.agent.collector.ip=${profiler.transport.grpc.collector.ip}
profiler.transport.grpc.agent.collector.port=9991
profiler.transport.grpc.agent.sender.channel.executor.queue.size=1000
profiler.transport.grpc.agent.sender.request.timeout.millis=6000
profiler.transport.grpc.agent.sender.keepalive.time.millis=30000
profiler.transport.grpc.agent.sender.keepalive.timeout.millis=60000
profiler.transport.grpc.agent.sender.connect.timeout.millis=3000
profiler.transport.grpc.agent.sender.headers.size.max=8K
profiler.transport.grpc.agent.sender.message.inbound.size.max=4M
profiler.transport.grpc.agent.sender.flow-control.window.size=1M
profiler.transport.grpc.agent.sender.write.buffer.highwatermark=32M
profiler.transport.grpc.agent.sender.write.buffer.lowwatermark=16M

# Metadata
profiler.transport.grpc.metadata.collector.ip=${profiler.transport.grpc.collector.ip}
profiler.transport.grpc.metadata.collector.port=9991
profiler.transport.grpc.metadata.sender.executor.queue.size=1000
profiler.transport.grpc.metadata.sender.channel.executor.queue.size=1000
profiler.transport.grpc.metadata.sender.request.timeout.millis=6000
profiler.transport.grpc.metadata.sender.retry.max.count=3
profiler.transport.grpc.metadata.sender.retry.delay.millis=1000
profiler.transport.grpc.metadata.sender.keepalive.time.millis=30000
profiler.transport.grpc.metadata.sender.keepalive.timeout.millis=60000
profiler.transport.grpc.metadata.sender.connect.timeout.millis=3000
profiler.transport.grpc.metadata.sender.headers.size.max=8K
profiler.transport.grpc.metadata.sender.message.inbound.size.max=4M
profiler.transport.grpc.metadata.sender.flow-control.window.size=1M
profiler.transport.grpc.metadata.sender.write.buffer.highwatermark=32M
profiler.transport.grpc.metadata.sender.write.buffer.lowwatermark=16M

# Stat
profiler.transport.grpc.stat.collector.ip=${profiler.transport.grpc.collector.ip}
profiler.transport.grpc.stat.collector.port=9992
profiler.transport.grpc.stat.sender.executor.queue.size=1000
profiler.transport.grpc.stat.sender.channel.executor.queue.size=1000
profiler.transport.grpc.stat.sender.request.timeout.millis=6000
profiler.transport.grpc.stat.sender.keepalive.time.millis=30000
profiler.transport.grpc.stat.sender.keepalive.timeout.millis=60000
profiler.transport.grpc.stat.sender.connect.timeout.millis=3000
profiler.transport.grpc.stat.sender.headers.size.max=8K
profiler.transport.grpc.stat.sender.message.inbound.size.max=4M
profiler.transport.grpc.stat.sender.flow-control.window.size=1M
profiler.transport.grpc.stat.sender.write.buffer.highwatermark=32M
profiler.transport.grpc.stat.sender.write.buffer.lowwatermark=16M

# Span
profiler.transport.grpc.span.collector.ip=${profiler.transport.grpc.collector.ip}
profiler.transport.grpc.span.collector.port=9993
profiler.transport.grpc.span.sender.executor.queue.size=1000
profiler.transport.grpc.span.sender.channel.executor.queue.size=1000
profiler.transport.grpc.span.sender.request.timeout.millis=6000
profiler.transport.grpc.span.sender.keepalive.time.millis=30000
profiler.transport.grpc.span.sender.keepalive.timeout.millis=60000
profiler.transport.grpc.span.sender.connect.timeout.millis=3000
profiler.transport.grpc.span.sender.headers.size.max=8K
profiler.transport.grpc.span.sender.message.inbound.size.max=4M
profiler.transport.grpc.span.sender.flow-control.window.size=1M
profiler.transport.grpc.span.sender.write.buffer.highwatermark=32M
profiler.transport.grpc.span.sender.write.buffer.lowwatermark=16M
profiler.transport.grpc.span.sender.discardpolicy.logger.discard.ratelimit=1
profiler.transport.grpc.span.sender.discardpolicy.maxpendingthreshold=1024

# This configuration enable some function of netty
# Functions are available without the this configuration when using jdk8 and below,
profiler.system.property.io.netty.tryReflectionSetAccessible=true
# disable netty directbuffer
profiler.system.property.io.netty.noPreferDirect=false

###########################################################
# Thrift Configuration                                    #
###########################################################
profiler.collector.ip=127.0.0.1

# placeHolder support "${key}"
profiler.collector.span.ip=${profiler.collector.ip}
profiler.collector.span.port=9996

# placeHolder support "${key}"
profiler.collector.stat.ip=${profiler.collector.ip}
profiler.collector.stat.port=9995

# placeHolder support "${key}"
profiler.collector.tcp.ip=${profiler.collector.ip}
profiler.collector.tcp.port=9994

###########################################################
# Profiler Global Configuration                           # 
###########################################################
profiler.enable=true

# Application namespace
# Differentiate from external pinpoint agents. (e.g., com.pinpoint)
profiler.application.namespace=

profiler.interceptorregistry.size=8192

# Manually override jvm vendor name (Oracle, IBM, OpenJDK, etc)
# You probably won't ever need to set this value.
profiler.jvm.vendor.name=

# Manually override agent's OS name (MAC, Window, Linus, AIX, HP_UX, BSD)
# You probably won't ever need to set this value.(to collect file descriptor)
profiler.os.name=

# Interval (in milliseconds) at which agent stat data is collected. (default : 5000, min : 1000, max : 5000)
profiler.jvm.stat.collect.interval=5000
# Number of agent stat data sent to the collector in a single batch. (default : 6)
profiler.jvm.stat.batch.send.count=6

# Allow to add detailed collector's metrics
profiler.jvm.stat.collect.detailed.metrics=true

# Allow sampling.
profiler.sampling.enable=true

# 1 out of n transactions will be sampled where n is the rate. (1: 100%, 20: 5%)
profiler.sampling.rate=1

# Permits per second, if throughput is 0, it is unlimited.
# "New" is a transaction that is newly traced.
profiler.sampling.new.throughput=0
# "Continue" is a transaction that is already being tracked.
profiler.sampling.continue.throughput=0

# Allow buffering when flushing span to IO.
profiler.io.buffering.enable=true

# How many spans to store if buffering enabled.
profiler.io.buffering.buffersize=20

###########################################################
# Base Tcp Sender                                        #
###########################################################
# Allow TCP data command.
profiler.tcpdatasender.command.accept.enable=true
profiler.tcpdatasender.command.activethread.enable=true
profiler.tcpdatasender.command.activethread.count.enable=true
profiler.tcpdatasender.command.activethread.threaddump.enable=true
profiler.tcpdatasender.command.activethread.threadlightdump.enable=true

profiler.tcpdatasender.client.write.timeout=3000
profiler.tcpdatasender.client.request.timeout=3000
profiler.tcpdatasender.client.reconnect.interval=3000
profiler.tcpdatasender.client.ping.interval=300000
profiler.tcpdatasender.client.handshake.interval=60000

profiler.tcpdatasender.client.write.buffer.highwatermark=32m
profiler.tcpdatasender.client.write.buffer.lowwatermark=16m

# Interval to retry sending agent info. Unit is milliseconds.
profiler.agentInfo.send.retry.interval=300000


###########################################################
# Stat Data Sender                                        #
###########################################################
# Should keep in mind
# 1. Loadbancing : TCP transport load balancing is per connection.(UDP transport loadbalancing is per packet)
# 2. In unexpected situations, UDP has its own protection feature (like packet loss etc.), but tcp does not have such a feature. (We will add protection later)
profiler.statdatasender.transport.type=UDP

# These settings are active only when using UDP.
# Capacity of the StatDataSender write queue.
profiler.statdatasender.write.queue.size=5120
#profiler.statdatasender.socket.sendbuffersize=1048576
#profiler.statdatasender.socket.timeout=3000
profiler.statdatasender.chunk.size=16384
profiler.statdatasender.socket.type=OIO

# These settings are active only when using TCP.
profiler.statdatasender.write.buffer.highwatermark=16m
profiler.statdatasender.write.buffer.lowwatermark=8m


###########################################################
# Span Data Sender                                        #
###########################################################
# Should keep in mind
# 1. Loadbancing : TCP transport load balancing is per connection.(UDP transport loadbalancing is per packet)
# 2. In unexpected situations, UDP has its own protection feature (like packet loss etc.), but tcp does not have such a feature. (We will add protection later)
profiler.spandatasender.transport.type=UDP

# These settings are active only when using UDP.
# Capacity of the SpanDataSender write queue.
profiler.spandatasender.write.queue.size=5120
#profiler.spandatasender.socket.sendbuffersize=1048576
#profiler.spandatasender.socket.timeout=3000
profiler.spandatasender.chunk.size=16384
profiler.spandatasender.socket.type=OIO

# These settings are active only when using TCP.
profiler.spandatasender.write.buffer.highwatermark=16m
profiler.spandatasender.write.buffer.lowwatermark=8m


# Trace Agent active thread info.
profiler.pinpoint.activethread=true

# Trace DataSource
profiler.pinpoint.datasource=true

# Deadlock Monitor
profiler.monitor.deadlock.enable=true
profiler.monitor.deadlock.interval=60000

## Call Stack
# Set max depth, if -1 is unlimited and min is 2.
profiler.callstack.max.depth=64

# weather or not to propagate exceptions occurred at interceptor
profiler.interceptor.exception.propagate=false

# Allow bytecode framework (ASM only)
profiler.instrument.engine=ASM

# bytecode dump option
# java bytecode debug option
bytecode.dump.enable=false
#bytecode.dump.classlist=com.pinpoint.user.UserService,com.pinpoint.debug.TestClass
bytecode.dump.classlist=
bytecode.dump.bytecode=false
bytecode.dump.verify=false
bytecode.dump.asm=false

# Matcher
profiler.instrument.matcher.enable=true
# Matcher cache. max size is 64.
profiler.instrument.matcher.interface.cache.size=4
profiler.instrument.matcher.interface.cache.entry.size=16
profiler.instrument.matcher.annotation.cache.size=4
profiler.instrument.matcher.annotation.cache.entry.size=4
profiler.instrument.matcher.super.cache.size=4
profiler.instrument.matcher.super.cache.entry.size=4

# Lambda expressions.
profiler.lambda.expressions.support=true

# Proxy HTTP headers.
# Please see (https://github.com/naver/pinpoint/blob/master/doc/proxy-http-header.md) for more information.
profiler.proxy.http.header.enable=true

# HTTP status code with request failure.
# 1xx, 2xx, 3xx, 4xx, 5xx, 100, 101, 200, 201, ... 501, 502, 503, 504, 505
# e.g. profiler.http.status.code.errors=5xx, 401, 403
profiler.http.status.code.errors=5xx

###########################################################
# application type                                        # 
###########################################################
#profiler.applicationservertype=TOMCAT
#profiler.applicationservertype=BLOC

# Needs to be a comma separated list of plugin's groupId:artifactId
# Ex: com.navercorp.pinpoint:pinpoint-tomcat-plugin, com.navercorp.pinpoint:pinpoint-jboss-plugin
profiler.plugin.load.order=
profiler.plugin.disable=

###########################################################
# Thread
###########################################################
# which package of runnable(callable) instance can be thread plugin trace
# Set the package name to track
# eg) profiler.thread.match.package=com.company.shopping.cart
profiler.thread.match.package=